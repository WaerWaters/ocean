{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccb30bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_684/603789124.py:21: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # FinTabNet Training with GTE-Inspired Joint Detection Model\n",
    "# ## Objective: Train a single Detectron2 model to detect both tables and cells jointly,\n",
    "# ## incorporating a GTE-inspired cell containment loss.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Imports and Setup\n",
    "\n",
    "# %%\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from PIL import Image, ImageDraw\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import copy\n",
    "import sys\n",
    "import cv2 # Needed for cropping and visualization\n",
    "import random\n",
    "import logging # Import standard logging\n",
    "import fitz # Import PyMuPDF\n",
    "\n",
    "# Setup logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Detectron2 Imports ---\n",
    "from detectron2.structures import BoxMode, Instances, Boxes, ImageList, pairwise_iou\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor, hooks\n",
    "from detectron2.config import get_cfg, CfgNode, configurable\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import (\n",
    "    DatasetCatalog,\n",
    "    MetadataCatalog,\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    "    DatasetMapper,\n",
    "    detection_utils as utils,\n",
    "    transforms as T\n",
    ")\n",
    "from detectron2.data.detection_utils import SizeMismatchError # Import the specific error\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.modeling import (\n",
    "    build_model,\n",
    "    build_backbone,\n",
    "    build_proposal_generator,\n",
    "    build_roi_heads,\n",
    "    META_ARCH_REGISTRY,\n",
    "    ROI_HEADS_REGISTRY,\n",
    "    StandardROIHeads # Use standard ROI heads initially\n",
    ")\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.utils.events import EventStorage, get_event_storage\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode # For drawing predictions\n",
    "from detectron2.layers import cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c440ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Constants and Configuration Setup\n",
    "\n",
    "# %%\n",
    "# --- Constants ---\n",
    "# Define categories for the *joint* model\n",
    "# IMPORTANT: Ensure these IDs match the ones assigned during data conversion\n",
    "categories_joint = [\"table\", \"cell\"]\n",
    "TABLE_CAT_ID = 0\n",
    "CELL_CAT_ID = 1\n",
    "NUM_CLASSES = len(categories_joint)\n",
    "\n",
    "# Colors for visualization (BGR) - Ensure order matches categories_joint\n",
    "colors = [(0, 0, 255), (0, 255, 0)] # Red for table, Green for cell\n",
    "\n",
    "# --- Configuration ---\n",
    "# Recommend using FinTabNet.c if available, adjust paths accordingly\n",
    "# For now, using original paths provided\n",
    "BASE_DIR = 'fintabnet'\n",
    "#!! IMPORTANT: Ensure this points to the folder with ORIGINAL PDFs!!\n",
    "PDF_FOLDER = os.path.join(BASE_DIR, 'pdf')\n",
    "#!! Consider switching to FinTabNet.c JSONL files if available!!\n",
    "TRAIN_JSONL = os.path.join(BASE_DIR, 'FinTabNet_1.0.0_table_train.jsonl')\n",
    "VAL_JSONL = os.path.join(BASE_DIR, 'FinTabNet_1.0.0_table_val.jsonl')\n",
    "TEST_JSONL = os.path.join(BASE_DIR, 'FinTabNet_1.0.0_table_test.jsonl')\n",
    "\n",
    "# Cache directory for processed data (increment version if logic changes)\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"converted_cache_gte_v1\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(PDF_FOLDER, exist_ok=True) # Ensure PDF folder exists\n",
    "\n",
    "# Output directory for model checkpoints and logs\n",
    "OUTPUT_DIR = \"./output_fintabnet_gte_model_v1\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Base Model Config ---\n",
    "cfg = get_cfg()\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Use Faster R-CNN with ResNet-50 FPN as a baseline\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "\n",
    "# --- Dataset Configuration ---\n",
    "# Use FinTabNet.c names if switching dataset source\n",
    "cfg.DATASETS.TRAIN = (\"fintabnet_gte_train\",)\n",
    "cfg.DATASETS.TEST = (\"fintabnet_gte_val\",) # Use validation set for evaluation during training\n",
    "\n",
    "# --- Dataloader Config ---\n",
    "cfg.DATALOADER.NUM_WORKERS = 4 # Adjust based on system cores/memory\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = True # Keep default\n",
    "\n",
    "# --- Model Configuration ---\n",
    "cfg.MODEL.META_ARCHITECTURE = \"GTE_MetaArch\" # Register custom MetaArch\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES # Set to 2 (table, cell)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64 # Adjust based on GPU memory (default 512)\n",
    "\n",
    "# Anchor generator settings (adjust if needed, especially for small cells)\n",
    "# Consider adding smaller anchors or different aspect ratios if cell detection is poor\n",
    "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[8], [16], [32], [64], [128]] # Added smaller size\n",
    "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.25, 0.5, 1.0, 2.0, 4.0]] # Added wider ratio\n",
    "\n",
    "# --- Custom GTE Configuration ---\n",
    "cfg.MODEL.GTE = CfgNode()\n",
    "#!! CRITICAL HYPERPARAMETER: Tune this value!!\n",
    "cfg.MODEL.GTE.CONTAINMENT_LOSS_WEIGHT = 1.0 # Start with 1.0, experiment with 0.1, 0.5, 2.0, 5.0 etc.\n",
    "# Optional: Threshold for IoU to consider a cell 'contained' in loss calculation\n",
    "cfg.MODEL.GTE.CONTAINMENT_IOU_THRESH = 0.5\n",
    "\n",
    "# --- Solver Configuration ---\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2 # Adjust based on GPU memory (try 2, 4, 8, 16...)\n",
    "# Adjust LR based on batch size (Linear Scaling Rule: new_lr = base_lr * new_batch_size / base_batch_size)\n",
    "# Base LR for IMS_PER_BATCH=16 is often 0.02 for SGD\n",
    "cfg.SOLVER.BASE_LR = 0.005 # Example for IMS_PER_BATCH=4 (0.02 * 4 / 16 = 0.005) - TUNE THIS\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "# Increase iterations significantly for large datasets like FinTabNet\n",
    "cfg.SOLVER.MAX_ITER = 90000 # Example: ~10 epochs if dataset size ~70k and batch size 4\n",
    "# Adjust steps based on MAX_ITER (e.g., decay at 66% and 88% of total iterations)\n",
    "cfg.SOLVER.STEPS = (120000, 160000)\n",
    "cfg.SOLVER.GAMMA = 0.1 # LR decay factor\n",
    "cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "cfg.SOLVER.WARMUP_FACTOR = 1.0 / cfg.SOLVER.WARMUP_ITERS # Standard linear warmup\n",
    "cfg.SOLVER.WEIGHT_DECAY = 0.0001\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 5000 # Save checkpoints less frequently for long runs\n",
    "cfg.TEST.EVAL_PERIOD = 200 # Evaluate on validation set periodically\n",
    "\n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "# Freeze config to prevent accidental changes\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:17:56,136 - INFO - Using PDF folder: fintabnet/pdf\n",
      "2025-04-29 00:17:56,137 - INFO - Using Cache folder: fintabnet/converted_cache_gte_v1\n",
      "2025-04-29 00:17:56,139 - INFO - Loading cached GTE joint dataset 'train' from fintabnet/converted_cache_gte_v1...\n",
      "2025-04-29 00:18:03,222 - INFO - Loaded 47985 records.\n",
      "2025-04-29 00:18:03,222 - ERROR - Error loading cache from fintabnet/converted_cache_gte_v1/converted_train_gte_joint.pkl: Cache item not dict.. Re-processing.\n",
      "2025-04-29 00:18:03,223 - INFO - Parsing and converting GTE joint dataset 'train' from fintabnet/FinTabNet_1.0.0_table_train.jsonl...\n",
      "2025-04-29 00:18:03,223 - INFO - Parsing FinTabNet_1.0.0_table_train.jsonl...\n",
      "2025-04-29 00:18:18,943 - INFO - Finished parsing FinTabNet_1.0.0_table_train.jsonl. Lines: 61801, Parsed Records with Annotations: 48001, Skipped (Missing/Unreadable PNG): 0\n",
      "2025-04-29 00:18:18,947 - INFO - Converting data to Detectron2 format (Joint GTE) with DPI scaling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 47993/48001 | Records: 47977 | Skipped (Img Err): 0 | Skipped (PDF Err): 0   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:18:54,247 - INFO - Finished Detectron2 conversion (Joint GTE). Created 47985 records. Skipped images: 0, Skipped PDFs: 0\n",
      "2025-04-29 00:18:54,330 - INFO - Saving converted GTE joint dataset 'train' to cache: fintabnet/converted_cache_gte_v1/converted_train_gte_joint.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 48001/48001 | Records: 47985 | Skipped (Img Err): 0 | Skipped (PDF Err): 0   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:19:01,081 - INFO - Loading cached GTE joint dataset 'val' from fintabnet/converted_cache_gte_v1...\n",
      "2025-04-29 00:19:01,307 - INFO - Loaded 5943 records.\n",
      "2025-04-29 00:19:01,308 - ERROR - Error loading cache from fintabnet/converted_cache_gte_v1/converted_val_gte_joint.pkl: Cache item not dict.. Re-processing.\n",
      "2025-04-29 00:19:01,309 - INFO - Parsing and converting GTE joint dataset 'val' from fintabnet/FinTabNet_1.0.0_table_val.jsonl...\n",
      "2025-04-29 00:19:01,310 - INFO - Parsing FinTabNet_1.0.0_table_val.jsonl...\n",
      "2025-04-29 00:19:02,564 - INFO - Finished parsing FinTabNet_1.0.0_table_val.jsonl. Lines: 7191, Parsed Records with Annotations: 5943, Skipped (Missing/Unreadable PNG): 0\n",
      "2025-04-29 00:19:02,565 - INFO - Converting data to Detectron2 format (Joint GTE) with DPI scaling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 5755/5943 | Records: 5755 | Skipped (Img Err): 0 | Skipped (PDF Err): 0   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:19:05,086 - INFO - Finished Detectron2 conversion (Joint GTE). Created 5943 records. Skipped images: 0, Skipped PDFs: 0\n",
      "2025-04-29 00:19:05,087 - INFO - Saving converted GTE joint dataset 'val' to cache: fintabnet/converted_cache_gte_v1/converted_val_gte_joint.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 5943/5943 | Records: 5943 | Skipped (Img Err): 0 | Skipped (PDF Err): 0   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:19:05,354 - INFO - Loading cached GTE joint dataset 'test' from fintabnet/converted_cache_gte_v1...\n",
      "2025-04-29 00:19:05,513 - INFO - Loaded 5903 records.\n",
      "2025-04-29 00:19:05,514 - ERROR - Error loading cache from fintabnet/converted_cache_gte_v1/converted_test_gte_joint.pkl: Cache item not dict.. Re-processing.\n",
      "2025-04-29 00:19:05,514 - INFO - Parsing and converting GTE joint dataset 'test' from fintabnet/FinTabNet_1.0.0_table_test.jsonl...\n",
      "2025-04-29 00:19:05,515 - INFO - Parsing FinTabNet_1.0.0_table_test.jsonl...\n",
      "2025-04-29 00:19:07,912 - INFO - Finished parsing FinTabNet_1.0.0_table_test.jsonl. Lines: 7085, Parsed Records with Annotations: 5903, Skipped (Missing/Unreadable PNG): 0\n",
      "2025-04-29 00:19:07,913 - INFO - Converting data to Detectron2 format (Joint GTE) with DPI scaling...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 5633/5903 | Records: 5633 | Skipped (Img Err): 0 | Skipped (PDF Err): 0   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:19:11,146 - INFO - Finished Detectron2 conversion (Joint GTE). Created 5903 records. Skipped images: 0, Skipped PDFs: 0\n",
      "2025-04-29 00:19:11,149 - INFO - Saving converted GTE joint dataset 'test' to cache: fintabnet/converted_cache_gte_v1/converted_test_gte_joint.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting: 5903/5903 | Records: 5903 | Skipped (Img Err): 0 | Skipped (PDF Err): 0   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:19:11,484 - INFO - Registering fintabnet_gte_train with 47985 records.\n",
      "2025-04-29 00:19:11,485 - INFO - Registering fintabnet_gte_val with 5943 records.\n",
      "2025-04-29 00:19:11,485 - INFO - Registering fintabnet_gte_test with 5903 records.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registering GTE joint datasets...\n",
      "Registered datasets: ['coco_2014_train', 'coco_2014_val', 'coco_2014_minival', 'coco_2014_valminusminival', 'coco_2017_train', 'coco_2017_val', 'coco_2017_test', 'coco_2017_test-dev', 'coco_2017_val_100', 'keypoints_coco_2014_train', 'keypoints_coco_2014_val', 'keypoints_coco_2014_minival', 'keypoints_coco_2014_valminusminival', 'keypoints_coco_2017_train', 'keypoints_coco_2017_val', 'keypoints_coco_2017_val_100', 'coco_2017_train_panoptic_separated', 'coco_2017_train_panoptic_stuffonly', 'coco_2017_train_panoptic', 'coco_2017_val_panoptic_separated', 'coco_2017_val_panoptic_stuffonly', 'coco_2017_val_panoptic', 'coco_2017_val_100_panoptic_separated', 'coco_2017_val_100_panoptic_stuffonly', 'coco_2017_val_100_panoptic', 'lvis_v1_train', 'lvis_v1_val', 'lvis_v1_test_dev', 'lvis_v1_test_challenge', 'lvis_v0.5_train', 'lvis_v0.5_val', 'lvis_v0.5_val_rand_100', 'lvis_v0.5_test', 'lvis_v0.5_train_cocofied', 'lvis_v0.5_val_cocofied', 'cityscapes_fine_instance_seg_train', 'cityscapes_fine_sem_seg_train', 'cityscapes_fine_instance_seg_val', 'cityscapes_fine_sem_seg_val', 'cityscapes_fine_instance_seg_test', 'cityscapes_fine_sem_seg_test', 'cityscapes_fine_panoptic_train', 'cityscapes_fine_panoptic_val', 'voc_2007_trainval', 'voc_2007_train', 'voc_2007_val', 'voc_2007_test', 'voc_2012_trainval', 'voc_2012_train', 'voc_2012_val', 'ade20k_sem_seg_train', 'ade20k_sem_seg_val', 'fintabnet_gte_train', 'fintabnet_gte_val', 'fintabnet_gte_test']\n",
      "Train metadata: Metadata(name='fintabnet_gte_train', thing_classes=['table', 'cell'], thing_colors=[(0, 0, 255), (0, 255, 0)])\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Data Loading and Preparation (with DPI Scaling)\n",
    "\n",
    "# %%\n",
    "# Keep the parse_fintabnet_jsonl function as is (from previous code)\n",
    "def parse_fintabnet_jsonl(jsonl_path, data_folder):\n",
    "    \"\"\"Parses FinTabNet JSONL file. Expects PNGs in data_folder.\"\"\"\n",
    "    images_data = {}\n",
    "    if not os.path.exists(jsonl_path):\n",
    "        logging.error(f\"JSONL file not found: {jsonl_path}\")\n",
    "        return {}\n",
    "    logging.info(f\"Parsing {os.path.basename(jsonl_path)}...\")\n",
    "    line_count, parsed_count, skipped_missing_file = 0, 0, 0\n",
    "    processed_files = set() # Track processed PDF filenames to count records correctly\n",
    "\n",
    "    with open(jsonl_path, 'r') as fp:\n",
    "        for line in fp:\n",
    "            line_count += 1\n",
    "            try:\n",
    "                sample = json.loads(line)\n",
    "                filename_pdf = sample['filename']\n",
    "                png_filename = filename_pdf.replace(\".pdf\", \".png\")\n",
    "                png_filepath = os.path.join(data_folder, png_filename)\n",
    "\n",
    "                if not os.path.exists(png_filepath):\n",
    "                    if skipped_missing_file % 1000 == 0:\n",
    "                         logging.warning(f\"PNG file not found (logged once per 1000): {png_filepath}\")\n",
    "                    skipped_missing_file += 1\n",
    "                    continue\n",
    "\n",
    "                # Use filename_pdf as the primary key\n",
    "                if filename_pdf not in images_data:\n",
    "                    try:\n",
    "                        with Image.open(png_filepath) as img:\n",
    "                            width, height = img.size\n",
    "                    except Exception as img_e:\n",
    "                        logging.error(f\"Failed to open/read dimensions for {png_filepath}: {img_e}\")\n",
    "                        skipped_missing_file += 1\n",
    "                        continue\n",
    "                    images_data[filename_pdf] = {'filepath_png': png_filepath, 'width': width, 'height': height, 'annotations':[]}\n",
    "\n",
    "                annotations = images_data[filename_pdf]['annotations']\n",
    "                added_annotation = False\n",
    "\n",
    "                # Check if it's a table annotation entry\n",
    "                # Heuristic: if 'html' key exists OR it's a simple entry (filename, bbox, split)\n",
    "                is_table_entry = 'html' in sample or len(sample.keys()) <= 3\n",
    "\n",
    "                if \"bbox\" in sample and is_table_entry:\n",
    "                    annotations.append({\"category_id\": TABLE_CAT_ID, \"bbox\": sample[\"bbox\"], \"is_table\": True}) # Use TABLE_CAT_ID\n",
    "                    added_annotation = True\n",
    "\n",
    "                # Check for cell annotations within the html structure\n",
    "                if \"html\" in sample and \"cells\" in sample[\"html\"]:\n",
    "                    for token in sample[\"html\"][\"cells\"]:\n",
    "                        if \"bbox\" in token:\n",
    "                            annotations.append({\"category_id\": CELL_CAT_ID, \"bbox\": token[\"bbox\"], \"is_table\": False}) # Use CELL_CAT_ID\n",
    "                            added_annotation = True\n",
    "\n",
    "                if added_annotation:\n",
    "                    processed_files.add(filename_pdf)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.warning(f\"Warning: Invalid JSON in line {line_count} in {jsonl_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Warning: Error processing line {line_count} in {jsonl_path}: {e}\")\n",
    "\n",
    "    final_images_data = {k: v for k, v in images_data.items() if v.get('annotations')}\n",
    "    parsed_count = len(final_images_data)\n",
    "\n",
    "    logging.info(f\"Finished parsing {os.path.basename(jsonl_path)}. Lines: {line_count}, Parsed Records with Annotations: {parsed_count}, Skipped (Missing/Unreadable PNG): {skipped_missing_file}\")\n",
    "    return final_images_data\n",
    "\n",
    "# %%\n",
    "# Modified conversion function to produce a single dataset dict list\n",
    "# with correctly scaled coordinates for both tables and cells.\n",
    "def convert_to_detectron_gte(images_dict, pdf_base_folder):\n",
    "    \"\"\"\n",
    "    Converts parsed data to Detectron2 format for the GTE joint model,\n",
    "    correctly scaling PDF coordinates to rendered image pixel coordinates.\n",
    "\n",
    "    Args:\n",
    "        images_dict (dict): Dictionary mapping PDF filenames to parsed data.\n",
    "        pdf_base_folder (str): Base directory containing original PDF files.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dataset dictionaries for Detectron2.\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    file_count = len(images_dict)\n",
    "    processed_count, skipped_img_error, skipped_pdf_error = 0, 0, 0\n",
    "    logging.info(\"Converting data to Detectron2 format (Joint GTE) with DPI scaling...\")\n",
    "\n",
    "    sorted_items = sorted(images_dict.items())\n",
    "\n",
    "    for idx, (filename_pdf, data) in enumerate(sorted_items):\n",
    "        processed_count += 1\n",
    "        png_path = data.get('filepath_png')\n",
    "        rendered_width = data.get('width')\n",
    "        rendered_height = data.get('height')\n",
    "        pdf_path = os.path.join(pdf_base_folder, filename_pdf)\n",
    "\n",
    "        if not png_path or not rendered_width or not rendered_height:\n",
    "            logging.warning(f\"Missing essential PNG data for {filename_pdf}. Skipping.\")\n",
    "            skipped_img_error += 1\n",
    "            continue\n",
    "\n",
    "        if not os.path.exists(pdf_path):\n",
    "            logging.warning(f\"Original PDF file not found at {pdf_path}. Skipping.\")\n",
    "            skipped_pdf_error += 1\n",
    "            continue\n",
    "\n",
    "        # --- Get PDF Page Dimensions and Calculate Scaling ---\n",
    "        try:\n",
    "            with fitz.open(pdf_path) as doc:\n",
    "                if not doc or len(doc) == 0: raise ValueError(\"PDF empty.\")\n",
    "                page = doc.load_page(0) # Assuming first page\n",
    "                pdf_rect = page.rect\n",
    "                pdf_page_width = pdf_rect.width\n",
    "                pdf_page_height = pdf_rect.height\n",
    "                if pdf_page_width <= 0 or pdf_page_height <= 0: raise ValueError(\"Invalid PDF dims.\")\n",
    "                scale_x = rendered_width / pdf_page_width\n",
    "                scale_y = rendered_height / pdf_page_height\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing PDF {pdf_path}: {e}. Skipping.\")\n",
    "            skipped_pdf_error += 1\n",
    "            continue\n",
    "        # --- ---\n",
    " \n",
    "        record = {\n",
    "            \"file_name\": png_path,\n",
    "            \"image_id\": idx,\n",
    "            \"height\": rendered_height,\n",
    "            \"width\": rendered_width,\n",
    "            \"annotations\":[]\n",
    "        }\n",
    "\n",
    "        # --- Process ALL Annotations (Scale and Flip) ---\n",
    "        for ann in data.get(\"annotations\",): # Added default empty list\n",
    "            if \"bbox\" not in ann or len(ann[\"bbox\"])!= 4: continue\n",
    "            category_id = ann.get(\"category_id\") # Should be TABLE_CAT_ID or CELL_CAT_ID\n",
    "            VALID_CATEGORY_IDS = [0,1,\"0\",\"1\"]\n",
    "            # *** CORRECTED CHECK ***\n",
    "            if category_id not in VALID_CATEGORY_IDS:\n",
    "                 logging.warning(f\"Skipping annotation with unexpected category_id {category_id} in {filename_pdf}\")\n",
    "                 continue\n",
    "\n",
    "            pdf_x1, pdf_y1, pdf_x2, pdf_y2 = ann[\"bbox\"]\n",
    "            if not all(isinstance(coord, (int, float)) for coord in [pdf_x1, pdf_y1, pdf_x2, pdf_y2]): continue\n",
    "\n",
    "            # Apply scaling and vertical flip\n",
    "            pixel_x1 = pdf_x1 * scale_x\n",
    "            pixel_x2 = pdf_x2 * scale_x\n",
    "            scaled_pdf_y1 = pdf_y1 * scale_y\n",
    "            scaled_pdf_y2 = pdf_y2 * scale_y\n",
    "            pixel_y1 = rendered_height - scaled_pdf_y2\n",
    "            pixel_y2 = rendered_height - scaled_pdf_y1\n",
    "\n",
    "            # Clip coordinates and ensure valid box\n",
    "            pixel_x1_c = max(0.0, min(pixel_x1, pixel_x2))\n",
    "            pixel_x2_c = min(float(rendered_width), max(pixel_x1, pixel_x2))\n",
    "            pixel_y1_c = max(0.0, min(pixel_y1, pixel_y2))\n",
    "            pixel_y2_c = min(float(rendered_height), max(pixel_y1, pixel_y2))\n",
    "\n",
    "            if pixel_x2_c <= pixel_x1_c or pixel_y2_c <= pixel_y1_c: continue # Skip invalid boxes\n",
    "\n",
    "            record[\"annotations\"].append({\n",
    "                \"bbox\": [pixel_x1_c, pixel_y1_c, pixel_x2_c, pixel_y2_c],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"category_id\": category_id, # Use the original category ID (0 or 1)\n",
    "            })\n",
    "\n",
    "        # Only add record if it has valid annotations\n",
    "        if record[\"annotations\"]:\n",
    "            dataset_dicts.append(record)\n",
    "\n",
    "        print(f\"Converting: {processed_count}/{file_count} | Records: {len(dataset_dicts)} | Skipped (Img Err): {skipped_img_error} | Skipped (PDF Err): {skipped_pdf_error}   \", end=\"\\r\")\n",
    "\n",
    "    print() # Newline after progress indicator\n",
    "    logging.info(f\"Finished Detectron2 conversion (Joint GTE). Created {len(dataset_dicts)} records. Skipped images: {skipped_img_error}, Skipped PDFs: {skipped_pdf_error}\")\n",
    "    return dataset_dicts\n",
    "\n",
    "# %%\n",
    "# Modified caching function\n",
    "def load_or_convert_gte_dataset(dataset_name, jsonl_path, image_folder, pdf_folder, cache_dir):\n",
    "    \"\"\"Loads GTE joint dataset from cache or parses/converts/caches.\"\"\"\n",
    "    # Use a distinct cache filename for the GTE joint data\n",
    "    cache_path = os.path.join(cache_dir, f\"converted_{dataset_name}_gte_joint.pkl\")\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        logging.info(f\"Loading cached GTE joint dataset '{dataset_name}' from {cache_dir}...\")\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            if not isinstance(data, list): raise TypeError(\"Cached data not list.\")\n",
    "            logging.info(f\"Loaded {len(data)} records.\")\n",
    "            if data and not isinstance(data, dict): raise TypeError(\"Cache item not dict.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading cache from {cache_path}: {e}. Re-processing.\")\n",
    "\n",
    "    logging.info(f\"Parsing and converting GTE joint dataset '{dataset_name}' from {jsonl_path}...\")\n",
    "    # Step 1: Parse the JSONL to get raw data including PNG paths and PDF coords\n",
    "    raw_data = parse_fintabnet_jsonl(jsonl_path, image_folder) # image_folder has PNGs\n",
    "    if not raw_data:\n",
    "        logging.error(f\"Error: No data parsed from {jsonl_path}. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Convert using the function that handles PDF scaling\n",
    "    converted_data = convert_to_detectron_gte(raw_data, pdf_folder) # pdf_folder has PDFs\n",
    "\n",
    "    # Step 3: Cache the result\n",
    "    if converted_data:\n",
    "        try:\n",
    "            logging.info(f\"Saving converted GTE joint dataset '{dataset_name}' to cache: {cache_path}\")\n",
    "            os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(converted_data, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving cache to {cache_path}: {e}\")\n",
    "    else:\n",
    "        logging.warning(f\"No data converted for {dataset_name}, skipping cache saving.\")\n",
    "\n",
    "    return converted_data\n",
    "\n",
    "# %%\n",
    "# --- Load and Register Datasets ---\n",
    "#!! IMPORTANT: Clear cache directory (CACHE_DIR) if conversion logic changed!!\n",
    "logging.info(f\"Using PDF folder: {PDF_FOLDER}\")\n",
    "logging.info(f\"Using Cache folder: {CACHE_DIR}\")\n",
    "\n",
    "# Load data using the new caching function\n",
    "train_dataset = load_or_convert_gte_dataset(\"train\", TRAIN_JSONL, PDF_FOLDER, PDF_FOLDER, CACHE_DIR)\n",
    "val_dataset = load_or_convert_gte_dataset(\"val\", VAL_JSONL, PDF_FOLDER, PDF_FOLDER, CACHE_DIR)\n",
    "test_dataset = load_or_convert_gte_dataset(\"test\", TEST_JSONL, PDF_FOLDER, PDF_FOLDER, CACHE_DIR)\n",
    "\n",
    "# Register the combined datasets\n",
    "def register_fintabnet_gte_datasets(train_data, val_data, test_data):\n",
    "    datasets_to_register = {\n",
    "        \"fintabnet_gte_train\": train_data,\n",
    "        \"fintabnet_gte_val\": val_data,\n",
    "        \"fintabnet_gte_test\": test_data,\n",
    "    }\n",
    "    print(\"\\nRegistering GTE joint datasets...\")\n",
    "    for name, data in datasets_to_register.items():\n",
    "        if name in DatasetCatalog.list():\n",
    "            logging.warning(f\"Dataset '{name}' already registered. Removing and re-registering.\")\n",
    "            DatasetCatalog.remove(name)\n",
    "            if name in MetadataCatalog.list():\n",
    "                MetadataCatalog.remove(name)\n",
    "\n",
    "        if data and isinstance(data, list) and len(data) > 0:\n",
    "            logging.info(f\"Registering {name} with {len(data)} records.\")\n",
    "            # Lambda function ensures data is loaded only when accessed\n",
    "            DatasetCatalog.register(name, lambda d=data: d)\n",
    "            # Set metadata (classes) for this dataset\n",
    "            MetadataCatalog.get(name).set(thing_classes=categories_joint)\n",
    "            MetadataCatalog.get(name).set(thing_colors=colors) # Optional: for visualization consistency\n",
    "        else:\n",
    "            logging.warning(f\"Skipping registration for {name} (dataset empty or invalid).\")\n",
    "\n",
    "register_fintabnet_gte_datasets(train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "# Verify registration (optional)\n",
    "print(\"Registered datasets:\", DatasetCatalog.list())\n",
    "if \"fintabnet_gte_train\" in MetadataCatalog.list():\n",
    "    metadata = MetadataCatalog.get(\"fintabnet_gte_train\")\n",
    "    print(\"Train metadata:\", metadata)\n",
    "else:\n",
    "    metadata = None # Needed for visualization later\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108990d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully unregistered GTE_MetaArch!\n"
     ]
    }
   ],
   "source": [
    "from detectron2.modeling.meta_arch import META_ARCH_REGISTRY\n",
    "\n",
    "if \"GTE_MetaArch\" in META_ARCH_REGISTRY._obj_map:\n",
    "    del META_ARCH_REGISTRY._obj_map[\"GTE_MetaArch\"]\n",
    "    print(\"Successfully unregistered GTE_MetaArch!\")\n",
    "else:\n",
    "    print(\"GTE_MetaArch not found in registry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Custom Meta Architecture ---\n",
    "@META_ARCH_REGISTRY.register()\n",
    "class GTE_MetaArch(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta architecture inspired by GTE, using standard Detectron2 components\n",
    "    but adding a custom containment loss during training.\n",
    "    Assumes a single ROI head predicts both table and cell classes.\n",
    "    \"\"\"\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        backbone,\n",
    "        proposal_generator,\n",
    "        roi_heads,\n",
    "        pixel_mean,\n",
    "        pixel_std,\n",
    "        input_format=None,\n",
    "        vis_period=0,\n",
    "        containment_loss_weight=1.0,\n",
    "        containment_iou_thresh=0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.proposal_generator = proposal_generator\n",
    "        self.roi_heads = roi_heads # Single head predicting both classes\n",
    "\n",
    "        self.input_format = input_format\n",
    "        self.vis_period = vis_period\n",
    "        if vis_period > 0:\n",
    "            assert input_format is not None, \"input_format is required for visualization!\"\n",
    "\n",
    "        self.register_buffer(\"pixel_mean\", torch.tensor(pixel_mean).view(-1, 1, 1), False)\n",
    "        self.register_buffer(\"pixel_std\", torch.tensor(pixel_std).view(-1, 1, 1), False)\n",
    "\n",
    "        # GTE specific parameters\n",
    "        self.containment_loss_weight = containment_loss_weight\n",
    "        self.containment_iou_thresh = containment_iou_thresh\n",
    "        logging.info(f\" Containment Loss Weight: {self.containment_loss_weight}\")\n",
    "        logging.info(f\" Containment IoU Threshold: {self.containment_iou_thresh}\")\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        backbone = build_backbone(cfg)\n",
    "        return {\n",
    "            \"backbone\": backbone,\n",
    "            \"proposal_generator\": build_proposal_generator(cfg, backbone.output_shape()),\n",
    "            \"roi_heads\": build_roi_heads(cfg, backbone.output_shape()), # Build the single ROI head\n",
    "            \"input_format\": cfg.INPUT.FORMAT,\n",
    "            \"vis_period\": cfg.VIS_PERIOD,\n",
    "            \"pixel_mean\": cfg.MODEL.PIXEL_MEAN,\n",
    "            \"pixel_std\": cfg.MODEL.PIXEL_STD,\n",
    "            \"containment_loss_weight\": cfg.MODEL.GTE.CONTAINMENT_LOSS_WEIGHT,\n",
    "            \"containment_iou_thresh\": cfg.MODEL.GTE.CONTAINMENT_IOU_THRESH,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.pixel_mean.device\n",
    "\n",
    "    def preprocess_image(self, batched_inputs):\n",
    "        \"\"\"Normalize, pad and batch the input images.\"\"\"\n",
    "        images = [x[\"image\"].to(self.device) for x in batched_inputs]\n",
    "        images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
    "        images = ImageList.from_tensors(images, self.backbone.size_divisibility)\n",
    "        return images\n",
    "    \n",
    "    def forward(self, batched_inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for training and inference.\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            return self.inference(batched_inputs)\n",
    "\n",
    "        # --- Check for Ground Truth ---\n",
    "        for x in batched_inputs:\n",
    "            if \"instances\" not in x:\n",
    "                 raise ValueError(\"Ground truth instances are required for training! Missing 'instances' key.\")\n",
    "\n",
    "        images = self.preprocess_image(batched_inputs)\n",
    "        gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        # --- RPN ---\n",
    "        if self.proposal_generator is not None:\n",
    "            if not gt_instances:\n",
    "                 raise ValueError(\"gt_instances are required for proposal_generator during training.\")\n",
    "            proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n",
    "        else:\n",
    "            assert \"proposals\" in batched_inputs, \"Proposals required if proposal_generator is None\"\n",
    "            proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n",
    "            proposal_losses = {}\n",
    "\n",
    "        # --- ROI Heads (Loss Calculation) ---\n",
    "        # Standard call to calculate losses\n",
    "        # The first returned value (often None or {}) is ignored here for loss calculation.\n",
    "        _, detector_losses = self.roi_heads(\n",
    "            images=images,\n",
    "            features=features,\n",
    "            proposals=proposals,\n",
    "            targets=gt_instances\n",
    "        )\n",
    "\n",
    "        # --- Combine Standard Losses ---\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "\n",
    "        # --- Custom Containment Loss ---\n",
    "        # Requires running ROI heads in inference mode to get predictions\n",
    "        if self.containment_loss_weight > 0:\n",
    "            # Store current training mode\n",
    "            original_training_mode = self.roi_heads.training\n",
    "            # Set to eval mode for inference pass\n",
    "            self.roi_heads.eval()\n",
    "            with torch.no_grad(): # No gradients needed for this inference pass\n",
    "                # Run inference pass using the same features and proposals\n",
    "                # targets=None indicates inference mode for ROI heads\n",
    "                pred_instances_list, _ = self.roi_heads(\n",
    "                    images=images,\n",
    "                    features=features,\n",
    "                    proposals=proposals,\n",
    "                    targets=None # Ensures inference path is taken\n",
    "                )\n",
    "            # Restore original training mode\n",
    "            self.roi_heads.train(original_training_mode)\n",
    "\n",
    "            # Now calculate containment loss using the predictions\n",
    "            if pred_instances_list is not None:\n",
    "                 # Check if the output is a list of Instances\n",
    "                 if isinstance(pred_instances_list, list) and all(isinstance(inst, Instances) for inst in pred_instances_list):\n",
    "                     containment_loss = self.compute_containment_loss(pred_instances_list)\n",
    "                     losses[\"loss_containment\"] = containment_loss * self.containment_loss_weight\n",
    "                 else:\n",
    "                     logging.warning(\"ROI heads inference output format unexpected for containment loss. Skipping loss.\")\n",
    "                     losses[\"loss_containment\"] = torch.tensor(0.0, device=self.device)\n",
    "            else:\n",
    "                 losses[\"loss_containment\"] = torch.tensor(0.0, device=self.device) # No predictions\n",
    "        else:\n",
    "            losses[\"loss_containment\"] = torch.tensor(0.0, device=self.device) # Loss weight is 0\n",
    "\n",
    "        # --- Visualization (Training) ---\n",
    "        # (Optional: visualize proposals or predictions if needed)\n",
    "        # if self.vis_period > 0:\n",
    "        #     storage = get_event_storage()\n",
    "        #     if storage.iter % self.vis_period == 0:\n",
    "        #         self.visualize_training(batched_inputs, proposals) # Assuming this method exists\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def inference(self, batched_inputs, detected_instances=None, do_postprocess=True):\n",
    "        \"\"\"\n",
    "        Run inference on the component models.\n",
    "        \"\"\"\n",
    "        assert not self.training\n",
    "\n",
    "        images = self.preprocess_image(batched_inputs) # Get the images object\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if detected_instances is None:\n",
    "            if self.proposal_generator is not None:\n",
    "                proposals, _ = self.proposal_generator(images, features, None)\n",
    "            else:\n",
    "                assert \"proposals\" in batched_inputs\n",
    "                proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n",
    "\n",
    "            # *** CORRECTED CALL: Pass 'images' as the first argument ***\n",
    "            results, _ = self.roi_heads(images, features, proposals, None)\n",
    "            # *** END CORRECTION ***\n",
    "        else:\n",
    "            # NOTE: If you use the forward_with_given_boxes path,\n",
    "            # ensure it also receives the correct arguments (likely including 'images').\n",
    "            # For simplicity, let's assume the standard path is used for now.\n",
    "            detected_instances = [x.to(self.device) for x in detected_instances]\n",
    "            # Using standard path even if instances are provided (might need adjustment if specific logic is needed)\n",
    "            if self.proposal_generator is not None:\n",
    "                 proposals, _ = self.proposal_generator(images, features, None)\n",
    "            else:\n",
    "                 # Handle case where proposals are needed but not generated/provided\n",
    "                 raise ValueError(\"Proposal generator is None, but proposals not found in input for inference with detected_instances.\")\n",
    "\n",
    "            results, _ = self.roi_heads(images, features, proposals, targets=None) # Pass images here too\n",
    "\n",
    "\n",
    "        if do_postprocess:\n",
    "            assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n",
    "            # Assuming GTE_MetaArch has _postprocess defined correctly\n",
    "            return GTE_MetaArch._postprocess(results, batched_inputs, images.image_sizes)\n",
    "        else:\n",
    "            return results\n",
    "\n",
    "    @staticmethod\n",
    "    def _postprocess(instances, batched_inputs, image_sizes):\n",
    "        \"\"\"Rescale predictions to original image size.\"\"\"\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            instances, batched_inputs, image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size)\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"instances\": r})\n",
    "        return processed_results\n",
    "\n",
    "    def compute_containment_loss(self, pred_instances_list):\n",
    "        \"\"\"\n",
    "        Computes the GTE-inspired containment loss.\n",
    "\n",
    "        Args:\n",
    "            pred_instances_list (list[Instances]): Predicted Instances per image.\n",
    "                Instances contain 'pred_boxes' and 'pred_classes'.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Scalar containment loss for the batch.\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        num_images_processed = 0\n",
    "\n",
    "        for instances in pred_instances_list:\n",
    "            if len(instances) == 0:\n",
    "                continue\n",
    "\n",
    "            pred_boxes = instances.pred_boxes # Boxes object\n",
    "            pred_classes = instances.pred_classes # Tensor of class IDs\n",
    "\n",
    "            # Separate predicted tables and cells\n",
    "            table_indices = (pred_classes == TABLE_CAT_ID).nonzero().squeeze(1)\n",
    "            cell_indices = (pred_classes == CELL_CAT_ID).nonzero().squeeze(1)\n",
    "\n",
    "            if len(table_indices) == 0 or len(cell_indices) == 0:\n",
    "                continue # Need both tables and cells predicted in an image for this loss\n",
    "\n",
    "            pred_table_boxes = pred_boxes[table_indices] # Boxes object for tables\n",
    "            pred_cell_boxes = pred_boxes[cell_indices]   # Boxes object for cells\n",
    "\n",
    "            # Calculate pairwise IoU between predicted cells and predicted tables\n",
    "            # Shape: (num_pred_cells, num_pred_tables)\n",
    "            ious = pairwise_iou(pred_cell_boxes, pred_table_boxes)\n",
    "\n",
    "            # --- Loss Component 1: Penalize cells \"outside\" tables ---\n",
    "            # For each cell, find the max IoU with any table\n",
    "            max_iou_per_cell, _ = torch.max(ious, dim=1)\n",
    "            # Cells with low max IoU are considered \"outside\"\n",
    "            # Penalty could be 1 - max_iou, or based on a threshold\n",
    "            # Simple penalty: average (1 - max_iou) for cells below threshold\n",
    "            outside_penalty = 1.0 - max_iou_per_cell\n",
    "            # Apply penalty only if max IoU is below threshold (optional, can make loss less noisy)\n",
    "            # outside_penalty = outside_penalty[max_iou_per_cell < self.containment_iou_thresh]\n",
    "            loss_outside = outside_penalty.mean() if len(outside_penalty) > 0 else torch.tensor(0.0, device=self.device)\n",
    "\n",
    "\n",
    "            # --- Loss Component 2: Penalize tables \"not containing\" cells ---\n",
    "            # For each table, count how many cells have IoU > threshold with it\n",
    "            contained_mask = ious > self.containment_iou_thresh\n",
    "            cells_contained_per_table = torch.sum(contained_mask, dim=0).float() # Shape: (num_pred_tables,)\n",
    "\n",
    "            # Simple penalty: encourage tables to contain at least one cell\n",
    "            # Penalize tables with zero contained cells (using 1 / (count + eps) encourages higher counts)\n",
    "            # Avoid division by zero with a small epsilon\n",
    "            epsilon = 1e-6\n",
    "            table_penalty = 1.0 / (cells_contained_per_table + epsilon)\n",
    "            loss_table_containment = table_penalty.mean() if len(table_penalty) > 0 else torch.tensor(0.0, device=self.device)\n",
    "\n",
    "            # Combine penalties (simple sum for now, could be weighted)\n",
    "            image_loss = loss_outside + loss_table_containment\n",
    "            total_loss += image_loss\n",
    "            num_images_processed += 1\n",
    "\n",
    "        # Average loss over images that contributed\n",
    "        return total_loss / num_images_processed if num_images_processed > 0 else torch.tensor(0.0, device=self.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting GTE Joint Model Training ---\n",
      "Output directory: ./output_fintabnet_gte_model_v1\n",
      "Configured MAX_ITER: 90000\n",
      "Containment Loss Weight: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:40:31,123 - INFO -  Containment Loss Weight: 1.0\n",
      "2025-04-29 00:40:31,123 - INFO -  Containment IoU Threshold: 0.5\n",
      "2025-04-29 00:40:31,173 - INFO - Model:\n",
      "GTE_MetaArch(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2025-04-29 00:40:31,175 - INFO - [DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "2025-04-29 00:40:31,336 - INFO - Removed 0 images with no usable annotations. 47985 images left.\n",
      "2025-04-29 00:40:32,726 - INFO - Using training sampler TrainingSampler\n",
      "2025-04-29 00:40:32,727 - INFO - Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "2025-04-29 00:40:32,727 - INFO - Serializing 47985 elements to byte tensors and concatenating them all ...\n",
      "2025-04-29 00:40:39,991 - INFO - Serialized dataset takes 178.74 MiB\n",
      "2025-04-29 00:40:39,994 - INFO - Making batched data loader with batch_size=2\n",
      "2025-04-29 00:40:40,007 - WARNING - SOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "2025-04-29 00:40:40,146 - INFO - [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
      "2025-04-29 00:40:40,147 - INFO - [Checkpointer] Loading from /home/saint/.torch/iopath_cache/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
      "2025-04-29 00:40:40,248 - INFO - Reading a file from 'Detectron2 Model Zoo'\n",
      "2025-04-29 00:40:40,293 - WARNING - Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.weight' to the model due to incompatible shapes: (3, 256, 1, 1) in the checkpoint but (5, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,294 - WARNING - Skip loading parameter 'proposal_generator.rpn_head.objectness_logits.bias' to the model due to incompatible shapes: (3,) in the checkpoint but (5,) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,294 - WARNING - Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.weight' to the model due to incompatible shapes: (12, 256, 1, 1) in the checkpoint but (20, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,295 - WARNING - Skip loading parameter 'proposal_generator.rpn_head.anchor_deltas.bias' to the model due to incompatible shapes: (12,) in the checkpoint but (20,) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,295 - WARNING - Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,296 - WARNING - Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,297 - WARNING - Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,297 - WARNING - Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
      "2025-04-29 00:40:40,299 - WARNING - Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "2025-04-29 00:40:40,301 - INFO - Starting training from iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GTE Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 00:41:00,677 - INFO -  eta: 19:39:49  iter: 19  total_loss: 5.163e+05  loss_cls: 0.8325  loss_box_reg: 0.01731  loss_rpn_cls: 0.6661  loss_rpn_loc: 0.4972  loss_containment: 5.163e+05    time: 0.9504  last_time: 0.7525  data_time: 0.0647  last_data_time: 0.0055   lr: 9.9905e-05  max_mem: 6245M\n",
      "2025-04-29 00:41:20,682 - INFO -  eta: 19:41:39  iter: 39  total_loss: 1.701  loss_cls: 0.4184  loss_box_reg: 0.1037  loss_rpn_cls: 0.5153  loss_rpn_loc: 0.3322  loss_containment: 0    time: 0.9762  last_time: 0.7782  data_time: 0.0042  last_data_time: 0.0040   lr: 0.0001998  max_mem: 6245M\n",
      "2025-04-29 00:41:39,981 - INFO -  eta: 20:03:15  iter: 59  total_loss: 1.371  loss_cls: 0.2928  loss_box_reg: 0.1381  loss_rpn_cls: 0.4186  loss_rpn_loc: 0.466  loss_containment: 0    time: 0.9715  last_time: 0.7907  data_time: 0.0043  last_data_time: 0.0041   lr: 0.0002997  max_mem: 6245M\n",
      "2025-04-29 00:41:59,655 - INFO -  eta: 20:11:29  iter: 79  total_loss: 1.153  loss_cls: 0.2746  loss_box_reg: 0.1539  loss_rpn_cls: 0.2773  loss_rpn_loc: 0.34  loss_containment: 0    time: 0.9746  last_time: 0.6617  data_time: 0.0044  last_data_time: 0.0042   lr: 0.00039961  max_mem: 6245M\n",
      "2025-04-29 00:42:20,754 - INFO -  eta: 20:11:13  iter: 99  total_loss: 1.066  loss_cls: 0.2223  loss_box_reg: 0.2402  loss_rpn_cls: 0.2355  loss_rpn_loc: 0.3429  loss_containment: 0    time: 0.9909  last_time: 1.2680  data_time: 0.0043  last_data_time: 0.0041   lr: 0.00049951  max_mem: 6245M\n",
      "2025-04-29 00:42:39,501 - INFO -  eta: 20:00:18  iter: 119  total_loss: 0.9933  loss_cls: 0.1835  loss_box_reg: 0.2176  loss_rpn_cls: 0.1805  loss_rpn_loc: 0.3088  loss_containment: 0    time: 0.9817  last_time: 0.6650  data_time: 0.0042  last_data_time: 0.0037   lr: 0.00059941  max_mem: 6245M\n",
      "2025-04-29 00:42:59,030 - INFO -  eta: 19:56:31  iter: 139  total_loss: 1.022  loss_cls: 0.1665  loss_box_reg: 0.3803  loss_rpn_cls: 0.1263  loss_rpn_loc: 0.3521  loss_containment: 0    time: 0.9809  last_time: 0.6895  data_time: 0.0044  last_data_time: 0.0053   lr: 0.0006993  max_mem: 6245M\n",
      "2025-04-29 00:43:19,249 - INFO -  eta: 19:56:15  iter: 159  total_loss: 1.142  loss_cls: 0.1653  loss_box_reg: 0.4114  loss_rpn_cls: 0.08633  loss_rpn_loc: 0.3061  loss_containment: 0    time: 0.9846  last_time: 0.7727  data_time: 0.0043  last_data_time: 0.0039   lr: 0.0007992  max_mem: 6245M\n",
      "2025-04-29 00:43:37,277 - INFO -  eta: 19:54:23  iter: 179  total_loss: 2.092  loss_cls: 0.2427  loss_box_reg: 0.3714  loss_rpn_cls: 0.1262  loss_rpn_loc: 0.3716  loss_containment: 0    time: 0.9752  last_time: 0.8002  data_time: 0.0040  last_data_time: 0.0040   lr: 0.0008991  max_mem: 6245M\n",
      "2025-04-29 00:43:56,804 - INFO - [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "2025-04-29 00:43:56,805 - INFO - Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "2025-04-29 00:43:56,806 - INFO - Serializing 5943 elements to byte tensors and concatenating them all ...\n",
      "2025-04-29 00:43:57,107 - INFO - Serialized dataset takes 19.03 MiB\n",
      "2025-04-29 00:43:58,591 - INFO - Start inference on 5943 batches\n",
      "2025-04-29 00:44:01,484 - INFO - Inference done 11/5943. Dataloading: 0.0011 s/iter. Inference: 0.1838 s/iter. Eval: 0.0005 s/iter. Total: 0.1855 s/iter. ETA=0:18:20\n",
      "2025-04-29 00:44:06,497 - INFO - Inference done 37/5943. Dataloading: 0.0016 s/iter. Inference: 0.1884 s/iter. Eval: 0.0012 s/iter. Total: 0.1914 s/iter. ETA=0:18:50\n",
      "2025-04-29 00:44:11,632 - INFO - Inference done 65/5943. Dataloading: 0.0016 s/iter. Inference: 0.1847 s/iter. Eval: 0.0011 s/iter. Total: 0.1877 s/iter. ETA=0:18:23\n",
      "2025-04-29 00:44:16,803 - INFO - Inference done 91/5943. Dataloading: 0.0017 s/iter. Inference: 0.1881 s/iter. Eval: 0.0011 s/iter. Total: 0.1911 s/iter. ETA=0:18:38\n",
      "2025-04-29 00:44:21,823 - INFO - Inference done 117/5943. Dataloading: 0.0016 s/iter. Inference: 0.1886 s/iter. Eval: 0.0012 s/iter. Total: 0.1915 s/iter. ETA=0:18:35\n",
      "2025-04-29 00:44:26,932 - INFO - Inference done 143/5943. Dataloading: 0.0016 s/iter. Inference: 0.1896 s/iter. Eval: 0.0011 s/iter. Total: 0.1925 s/iter. ETA=0:18:36\n",
      "2025-04-29 00:44:32,109 - INFO - Inference done 171/5943. Dataloading: 0.0016 s/iter. Inference: 0.1884 s/iter. Eval: 0.0011 s/iter. Total: 0.1912 s/iter. ETA=0:18:23\n",
      "2025-04-29 00:44:37,210 - INFO - Inference done 198/5943. Dataloading: 0.0016 s/iter. Inference: 0.1881 s/iter. Eval: 0.0011 s/iter. Total: 0.1909 s/iter. ETA=0:18:16\n",
      "2025-04-29 00:44:42,229 - INFO - Inference done 223/5943. Dataloading: 0.0016 s/iter. Inference: 0.1892 s/iter. Eval: 0.0011 s/iter. Total: 0.1920 s/iter. ETA=0:18:18\n",
      "2025-04-29 00:44:47,409 - INFO - Inference done 249/5943. Dataloading: 0.0016 s/iter. Inference: 0.1900 s/iter. Eval: 0.0011 s/iter. Total: 0.1928 s/iter. ETA=0:18:17\n",
      "2025-04-29 00:44:52,476 - INFO - Inference done 276/5943. Dataloading: 0.0016 s/iter. Inference: 0.1895 s/iter. Eval: 0.0011 s/iter. Total: 0.1923 s/iter. ETA=0:18:09\n",
      "2025-04-29 00:44:53,954 - INFO - Overall training speed: 197 iterations in 0:03:12 (0.9777 s / it)\n",
      "2025-04-29 00:44:53,955 - INFO - Total training time: 0:04:10 (0:00:57 on hooks)\n",
      "2025-04-29 00:44:53,958 - INFO -  eta: 19:53:38  iter: 199  total_loss: 2.761  loss_cls: 0.1738  loss_box_reg: 0.3629  loss_rpn_cls: 0.109  loss_rpn_loc: 0.3043  loss_containment: 1.91    time: 0.9728  last_time: 1.0249  data_time: 0.0039  last_data_time: 0.0052   lr: 0.00099901  max_mem: 6245M\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGTE Model device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- GTE Joint Model Training Finished ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/defaults.py:520\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    Run training.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[1;32m    517\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mEXPECTED_RESULTS) \u001b[38;5;129;01mand\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m    523\u001b[0m             \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_last_eval_results\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    524\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo evaluation results obtained during training!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/train_loop.py:156\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbefore_step()\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_step()\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# due to exceptions.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/train_loop.py:190\u001b[0m, in \u001b[0;36mTrainerBase.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mafter_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[0;32m--> 190\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/hooks.py:556\u001b[0m, in \u001b[0;36mEvalHook.after_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_period \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m next_iter \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_period \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# do the last eval in after_train\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_iter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmax_iter:\n\u001b[0;32m--> 556\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/hooks.py:529\u001b[0m, in \u001b[0;36mEvalHook._do_eval\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_do_eval\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    533\u001b[0m             results, \u001b[38;5;28mdict\u001b[39m\n\u001b[1;32m    534\u001b[0m         ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEval function must return a dict. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(results)\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/defaults.py:489\u001b[0m, in \u001b[0;36mDefaultTrainer.build_hooks.<locals>.test_and_save_results\u001b[0;34m()\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtest_and_save_results\u001b[39m():\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_eval_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_eval_results\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/defaults.py:653\u001b[0m, in \u001b[0;36mDefaultTrainer.test\u001b[0;34m(cls, cfg, model, evaluators)\u001b[0m\n\u001b[1;32m    651\u001b[0m         results[dataset_name] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    652\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m results_i \u001b[38;5;241m=\u001b[39m \u001b[43minference_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m results[dataset_name] \u001b[38;5;241m=\u001b[39m results_i\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m comm\u001b[38;5;241m.\u001b[39mis_main_process():\n",
      "File \u001b[0;32m~/detectron2/detectron2/evaluation/evaluator.py:165\u001b[0m, in \u001b[0;36minference_on_dataset\u001b[0;34m(model, data_loader, evaluator, callbacks)\u001b[0m\n\u001b[1;32m    163\u001b[0m start_compute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mget(callbacks \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[0;32m--> 165\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mget(callbacks \u001b[38;5;129;01mor\u001b[39;00m {}, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)()\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 75\u001b[0m, in \u001b[0;36mGTE_MetaArch.forward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mForward pass for training and inference.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# --- Check for Ground Truth ---\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batched_inputs:\n",
      "Cell \u001b[0;32mIn[17], line 164\u001b[0m, in \u001b[0;36mGTE_MetaArch.inference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m         proposals, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/detectron2/detectron2/modeling/proposal_generator/rpn.py:452\u001b[0m, in \u001b[0;36mRPN.forward\u001b[0;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    images (ImageList): input images of length `N`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    loss: dict[Tensor] or None\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]\n\u001b[0;32m--> 452\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn_head(features)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/detectron2/detectron2/modeling/anchor_generator.py:230\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03m    features (list[Tensor]): list of backbone feature maps on which to generate anchors.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m        where Hi, Wi are resolution of the feature map divided by anchor stride.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m grid_sizes \u001b[38;5;241m=\u001b[39m [feature_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m--> 230\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grid_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_sizes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pyre-ignore\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Boxes(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m anchors_over_all_feature_maps]\n",
      "File \u001b[0;32m~/detectron2/detectron2/modeling/anchor_generator.py:174\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator._grid_anchors\u001b[0;34m(self, grid_sizes)\u001b[0m\n\u001b[1;32m    172\u001b[0m buffers: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_anchors\u001b[38;5;241m.\u001b[39mnamed_buffers()]\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, stride, base_anchors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grid_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides, buffers):\n\u001b[0;32m--> 174\u001b[0m     shift_x, shift_y \u001b[38;5;241m=\u001b[39m \u001b[43m_create_grid_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_anchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     shifts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((shift_x, shift_y, shift_x, shift_y), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    177\u001b[0m     anchors\u001b[38;5;241m.\u001b[39mappend((shifts\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m+\u001b[39m base_anchors\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m~/detectron2/detectron2/modeling/anchor_generator.py:43\u001b[0m, in \u001b[0;36m_create_grid_offsets\u001b[0;34m(size, stride, offset, target_device_tensor)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_grid_offsets\u001b[39m(\n\u001b[1;32m     40\u001b[0m     size: List[\u001b[38;5;28mint\u001b[39m], stride: \u001b[38;5;28mint\u001b[39m, offset: \u001b[38;5;28mfloat\u001b[39m, target_device_tensor: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[1;32m     41\u001b[0m ):\n\u001b[1;32m     42\u001b[0m     grid_height, grid_width \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m---> 43\u001b[0m     shifts_x \u001b[38;5;241m=\u001b[39m \u001b[43mmove_device_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_device_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     shifts_y \u001b[38;5;241m=\u001b[39m move_device_like(\n\u001b[1;32m     48\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(offset \u001b[38;5;241m*\u001b[39m stride, grid_height \u001b[38;5;241m*\u001b[39m stride, step\u001b[38;5;241m=\u001b[39mstride, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m     49\u001b[0m         target_device_tensor,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     52\u001b[0m     shift_y, shift_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(shifts_y, shifts_x)\n",
      "File \u001b[0;32m~/anaconda3/envs/detectron2_env/lib/python3.10/site-packages/torch/jit/_trace.py:1444\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[1;32m   1443\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[0;32m-> 1444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1446\u001b[0m     compiled_fn: Callable[P, R] \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/detectron2/detectron2/layers/wrappers.py:177\u001b[0m, in \u001b[0;36mmove_device_like\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_if_tracing\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmove_device_like\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, dst: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    Tracing friendly way to cast tensor to another tensor's device. Device will be treated\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    as constant during tracing, scripting the casting process as whole can workaround this issue.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Custom Model (GTE_MetaArch) and Trainer\n",
    "\n",
    "# --- Custom Trainer ---\n",
    "class GTETrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer that uses the GTE_MetaArch and potentially custom data loaders/evaluators.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def build_model(cls, cfg):\n",
    "        \"\"\"Builds the GTE_MetaArch.\"\"\"\n",
    "        model = META_ARCH_REGISTRY.get(cfg.MODEL.META_ARCHITECTURE)(cfg)\n",
    "        model.to(cfg.MODEL.DEVICE)\n",
    "        logging.info(f\"Model:\\n{model}\")\n",
    "        return model\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \"\"\"Builds the training data loader.\"\"\"\n",
    "        # Use standard DatasetMapper for now, assuming combined GT is handled\n",
    "        mapper = DatasetMapper(cfg, is_train=True)\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"Builds the evaluator for the validation set.\"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        # Use standard COCOEvaluator\n",
    "        # Ensure the inference output format matches what COCOEvaluator expects\n",
    "        return COCOEvaluator(dataset_name, output_dir=output_folder)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Training\n",
    "\n",
    "# %%\n",
    "# --- Start Training ---\n",
    "do_train = True\n",
    "if do_train:\n",
    "    print(f\"\\n--- Starting GTE Joint Model Training ---\")\n",
    "    print(f\"Output directory: {cfg.OUTPUT_DIR}\")\n",
    "    print(f\"Configured MAX_ITER: {cfg.SOLVER.MAX_ITER}\")\n",
    "    print(f\"Containment Loss Weight: {cfg.MODEL.GTE.CONTAINMENT_LOSS_WEIGHT}\")\n",
    "\n",
    "    # Check if the configured training dataset exists and is registered\n",
    "    if not cfg.DATASETS.TRAIN: # Check if the tuple is empty first\n",
    "        logging.error(\"No training dataset specified in cfg.DATASETS.TRAIN. Aborting training.\")\n",
    "        do_train = False # Prevent proceeding\n",
    "    else:\n",
    "        train_dataset_name_str = cfg.DATASETS.TRAIN[0] # Get the first element (the string name)\n",
    "\n",
    "        if train_dataset_name_str not in DatasetCatalog.list():\n",
    "            logging.error(f\"Training dataset '{train_dataset_name_str}' is not registered in DatasetCatalog. Aborting training.\")\n",
    "            do_train = False # Prevent proceeding\n",
    "        # Check if the actual data getter function returns something (is not empty)\n",
    "        # This ensures the data loading/conversion didn't fail silently earlier\n",
    "        elif not DatasetCatalog.get(train_dataset_name_str):\n",
    "            logging.error(f\"Training dataset '{train_dataset_name_str}' is registered but appears empty (data loading/conversion might have failed). Aborting training.\")\n",
    "            do_train = False # Prevent proceeding\n",
    "\n",
    "    # Proceed only if the checks passed\n",
    "    if do_train:\n",
    "        trainer = GTETrainer(cfg)\n",
    "        # Load last checkpoint if available, otherwise start from MODEL.WEIGHTS\n",
    "        trainer.resume_or_load(resume=True) # Set resume=True to continue training if checkpoint exists\n",
    "        print(f\"GTE Model device: {next(trainer.model.parameters()).device}\")\n",
    "        try:\n",
    "            trainer.train()\n",
    "            print(\"--- GTE Joint Model Training Finished ---\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred during GTE training: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "else:\n",
    "    print(\"\\nSkipping GTE Joint Model training (do_train=False).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957fa7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m predictor_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     45\u001b[0m gte_predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39mOUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(final_model_path):\n\u001b[1;32m     49\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInitializing GTEPredictor with weights from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Inference and Visualization\n",
    "\n",
    "# %%\n",
    "# --- Custom Predictor (Optional, DefaultPredictor might work) ---\n",
    "class GTEPredictor:\n",
    "    \"\"\"Simple predictor wrapper for the trained GTE model.\"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg.clone() # Clone cfg to avoid modifying original\n",
    "        self.cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "        # Set threshold for inference\n",
    "        self.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5 # Adjust as needed\n",
    "        self.cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = GTETrainer.build_model(self.cfg) # Build the custom model\n",
    "        self.model.eval()\n",
    "        checkpointer = DetectionCheckpointer(self.model)\n",
    "        checkpointer.load(self.cfg.MODEL.WEIGHTS)\n",
    "\n",
    "        # Get metadata for visualization\n",
    "        self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST if cfg.DATASETS.TEST else \"__unused\")\n",
    "        if not self.metadata.thing_classes:\n",
    "             logging.warning(\"Metadata missing thing_classes, setting default for GTE.\")\n",
    "             self.metadata.thing_classes = categories_joint\n",
    "             self.metadata.thing_colors = colors\n",
    "\n",
    "\n",
    "    def __call__(self, original_image_bgr):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image_bgr (np.ndarray): An image in BGR format.\n",
    "\n",
    "        Returns:\n",
    "            predictions (dict): The model's predictions in Detectron2 format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            height, width = original_image_bgr.shape[:2]\n",
    "            image = torch.as_tensor(original_image_bgr.astype(\"float32\").transpose(2, 0, 1))\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs]) # Run model's inference method\n",
    "            return predictions\n",
    "\n",
    "# %%\n",
    "# --- Initialize Predictor ---\n",
    "predictor_ready = False\n",
    "gte_predictor = None\n",
    "final_model_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "if os.path.exists(final_model_path):\n",
    "    logging.info(f\"\\nInitializing GTEPredictor with weights from {final_model_path}...\")\n",
    "    try:\n",
    "        predictor_cfg = cfg.clone() # Use the same config used for training\n",
    "        gte_predictor = GTEPredictor(predictor_cfg)\n",
    "        predictor_ready = True\n",
    "        logging.info(\"GTEPredictor initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"\\nError initializing GTEPredictor: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    logging.warning(f\"\\nCannot initialize predictor: Final model weights not found at {final_model_path}\")\n",
    "\n",
    "# %%\n",
    "# --- Evaluation ---\n",
    "# Run evaluation using the custom trainer's method\n",
    "if predictor_ready and cfg.DATASETS.TEST:\n",
    "    test_dataset_name = cfg.DATASETS.TEST\n",
    "    if test_dataset_name in DatasetCatalog.list():\n",
    "        logging.info(f\"\\nRunning evaluation on GTE test set ({test_dataset_name})...\")\n",
    "        eval_output_dir = os.path.join(cfg.OUTPUT_DIR, \"inference_test_final\")\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "        evaluator = GTETrainer.build_evaluator(cfg, test_dataset_name, output_folder=eval_output_dir)\n",
    "        test_loader = GTETrainer.build_test_loader(cfg, test_dataset_name)\n",
    "        try:\n",
    "            results = inference_on_dataset(gte_predictor.model, test_loader, evaluator)\n",
    "            print(\"\\n--- GTE Model Evaluation Results ---\")\n",
    "            print(results)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during GTE evaluation: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"\\nSkipping GTE evaluation: Test dataset '{test_dataset_name}' not found.\")\n",
    "else:\n",
    "    print(\"\\nSkipping GTE evaluation (predictor or test dataset not ready).\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# --- Visualization ---\n",
    "# Keep the visualize_predictions function, but ensure it uses the correct metadata\n",
    "def visualize_predictions(dataset, predictor, metadata, num_samples=5, title_prefix=\"Predictions\"):\n",
    "    \"\"\"Visualizes predictions from a predictor on a dataset.\"\"\"\n",
    "    if not dataset: logging.warning(f\"Cannot visualize: Dataset for '{title_prefix}' is empty.\"); return\n",
    "    if not predictor: logging.warning(f\"Cannot visualize: Predictor for '{title_prefix}' is None.\"); return\n",
    "    if not metadata: logging.warning(f\"Cannot visualize: Metadata for '{title_prefix}' is None.\"); return\n",
    "    if not hasattr(metadata, 'thing_classes') or not metadata.thing_classes:\n",
    "        logging.error(f\"Metadata for '{title_prefix}' missing 'thing_classes'. Cannot visualize.\"); return\n",
    "\n",
    "    logging.info(f\"\\nVisualizing {num_samples} {title_prefix}...\")\n",
    "    actual_num_samples = min(num_samples, len(dataset))\n",
    "    if actual_num_samples <= 0: logging.info(\"No samples in dataset.\"); return\n",
    "\n",
    "    try:\n",
    "        # Ensure dataset is a list of dicts before sampling\n",
    "        if isinstance(dataset, list) and all(isinstance(item, dict) for item in dataset):\n",
    "            indices = random.sample(range(len(dataset)), actual_num_samples)\n",
    "            samples = [dataset[i] for i in indices]\n",
    "        else:\n",
    "             logging.warning(\"Dataset is not list of dicts, attempting to take first samples.\")\n",
    "             samples = [item for i, item in enumerate(dataset) if i < actual_num_samples and isinstance(item, dict)]\n",
    "             if not samples: raise ValueError(\"Could not get valid samples.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Cannot sample from dataset for '{title_prefix}': {e}\"); return\n",
    "\n",
    "    for sample in samples:\n",
    "        img_path = sample.get(\"file_name\")\n",
    "        if not img_path or not os.path.exists(img_path):\n",
    "            logging.warning(f\"Image path invalid in sample: {sample.get('image_id', 'Unknown ID')}. Skipping.\"); continue\n",
    "        try:\n",
    "            img_bgr = cv2.imread(img_path)\n",
    "            if img_bgr is None: logging.warning(f\"Cannot read image: {img_path}. Skipping.\"); continue\n",
    "        except Exception as read_e:\n",
    "            logging.warning(f\"Exception reading image {img_path}: {read_e}. Skipping.\"); continue\n",
    "\n",
    "        logging.info(f\"Running predictor for: {os.path.basename(img_path)}\")\n",
    "        try:\n",
    "            outputs = predictor(img_bgr) # Use the predictor instance directly\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during prediction for {os.path.basename(img_path)}: {e}\"); continue\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "        v = Visualizer(img_rgb, metadata, scale=1.0, instance_mode=ColorMode.IMAGE) # Use provided metadata\n",
    "\n",
    "        out_vis = None\n",
    "        if isinstance(outputs, dict) and \"instances\" in outputs:\n",
    "            instances = outputs[\"instances\"].to(\"cpu\")\n",
    "            if len(instances) > 0:\n",
    "                out_vis = v.draw_instance_predictions(instances)\n",
    "            else:\n",
    "                logging.info(f\"No instances detected for {os.path.basename(img_path)}.\")\n",
    "        else:\n",
    "            logging.warning(f\"Predictor output format not recognized for visualization.\"); continue\n",
    "\n",
    "        if out_vis is not None:\n",
    "            output_image = out_vis.get_image()\n",
    "            plt.figure(figsize=(15, 15)); plt.imshow(output_image)\n",
    "            plt.title(f\"{title_prefix} - {os.path.basename(img_path)}\"); plt.axis('off'); plt.show()\n",
    "        else: # Show original image if no detections\n",
    "            plt.figure(figsize=(12, 12)); plt.imshow(img_rgb)\n",
    "            plt.title(f\"{title_prefix} - {os.path.basename(img_path)} (No Detections)\"); plt.axis('off'); plt.show()\n",
    "\n",
    "# %%\n",
    "# --- Run Visualization ---\n",
    "if predictor_ready and test_dataset:\n",
    "    # Get metadata associated with the registered test dataset\n",
    "    test_metadata = MetadataCatalog.get(cfg.DATASETS.TEST) if cfg.DATASETS.TEST else None\n",
    "    if test_metadata and test_metadata.thing_classes:\n",
    "         visualize_predictions(dataset=test_dataset, predictor=gte_predictor, metadata=test_metadata, num_samples=10, title_prefix=\"GTE Joint Model Predictions\")\n",
    "    else:\n",
    "         print(\"\\nSkipping visualization: Metadata not found or incomplete for test dataset.\")\n",
    "else:\n",
    "    print(\"\\nSkipping visualization (predictor or test dataset not ready).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de4a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
